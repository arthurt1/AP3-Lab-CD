{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arthurt1/AP3-Lab-CD/blob/artur-garcia/An%C3%A1lise-collab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas requests beautifulsoup4"
      ],
      "metadata": {
        "id": "B98eIcQ0hZ3v",
        "outputId": "f5cb19d9-ed3b-4f7d-aaf6-f7159d72228f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import os"
      ],
      "metadata": {
        "id": "1iKxwN3IiiHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Repositório e pasta onde os arquivos CSV estão localizados\n",
        "repo_owner = \"arthurt1\"\n",
        "repo_name = \"AP3-Lab-CD\"\n",
        "folder_path = \"Datasets\"\n",
        "\n",
        "# URL da API do GitHub para listar o conteúdo da pasta\n",
        "api_url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contents/{folder_path}\"\n",
        "\n",
        "# Criar a pasta local para armazenar os arquivos baixados\n",
        "os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "# Fazendo requisição à API do GitHub\n",
        "response = requests.get(api_url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    files = response.json()\n",
        "    for file in files:\n",
        "        if file[\"name\"].endswith(\".csv\"):  # Verifica se o arquivo é CSV\n",
        "            download_url = file[\"download_url\"]  # URL direta para baixar o arquivo\n",
        "            file_path = os.path.join(folder_path, file[\"name\"])\n",
        "\n",
        "            # Faz o download do arquivo\n",
        "            file_response = requests.get(download_url)\n",
        "            with open(file_path, \"wb\") as f:\n",
        "                f.write(file_response.content)\n",
        "\n",
        "            print(f\"Arquivo {file['name']} baixado com sucesso.\")\n",
        "else:\n",
        "    print(\"Erro ao acessar a API do GitHub. Verifique se o repositório e a pasta estão corretos.\")\n"
      ],
      "metadata": {
        "id": "8PNWx4aTjd-e",
        "outputId": "8e2e8cd9-4536-4271-97bb-9cce6b7ae626",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo Areninhas.csv baixado com sucesso.\n",
            "Arquivo Assentamentos_Precários.csv baixado com sucesso.\n",
            "Arquivo Bairros_de_Fortaleza.csv baixado com sucesso.\n",
            "Arquivo CAPS-_Centros_de_Assistência_Psicossocial.csv baixado com sucesso.\n",
            "Arquivo Densidade_Populacional_por_Bairros_(km²).csv baixado com sucesso.\n",
            "Arquivo Domicílios.csv baixado com sucesso.\n",
            "Arquivo Equipamentos_de_Assistência_Social.csv baixado com sucesso.\n",
            "Arquivo Equipamentos_de_Saúde.csv baixado com sucesso.\n",
            "Arquivo Gravidez_na_Adolescência.csv baixado com sucesso.\n",
            "Arquivo IDH-_Classificação.csv baixado com sucesso.\n",
            "Arquivo Pontos_de_Ônibus.csv baixado com sucesso.\n",
            "Arquivo Rede_Cuca.csv baixado com sucesso.\n",
            "Arquivo Rede_Juv.csv baixado com sucesso.\n",
            "Arquivo Rede_de_Abastecimento_de_Água.csv baixado com sucesso.\n",
            "Arquivo Rede_de_Esgoto.csv baixado com sucesso.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjwem2nXVx-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a865d97-c4d9-4a77-cd0e-73cddebe7889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Bairros_de_Fortaleza.csv carregado com sucesso.\n",
            "✔ IDH-_Classificação.csv carregado com sucesso.\n",
            "Erro ao processar Rede_de_Abastecimento_de_Água.csv: No columns to parse from file\n",
            "✔ Rede_Cuca.csv carregado com sucesso.\n",
            "✔ Densidade_Populacional_por_Bairros_(km²).csv carregado com sucesso.\n",
            "✔ Areninhas.csv carregado com sucesso.\n",
            "✔ Rede_Juv.csv carregado com sucesso.\n",
            "Erro ao processar Rede_de_Esgoto.csv: No columns to parse from file\n",
            "✔ Pontos_de_Ônibus.csv carregado com sucesso.\n",
            "✔ Assentamentos_Precários.csv carregado com sucesso.\n",
            "✔ CAPS-_Centros_de_Assistência_Psicossocial.csv carregado com sucesso.\n",
            "✔ Equipamentos_de_Assistência_Social.csv carregado com sucesso.\n",
            "✔ Equipamentos_de_Saúde.csv carregado com sucesso.\n",
            "✔ Gravidez_na_Adolescência.csv carregado com sucesso.\n",
            "✔ Domicílios.csv carregado com sucesso.\n",
            "\n",
            "✅ Cabeçalhos salvos em /content/headers.json\n",
            "Processo concluído.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "def main():\n",
        "    # Define o diretório de datasets\n",
        "    datasets_dir = Path(\"/content/Datasets\")\n",
        "\n",
        "    # Verifica se o diretório existe\n",
        "    if not datasets_dir.exists():\n",
        "        print(f\"O diretório {datasets_dir} não existe.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Lista para armazenar os cabeçalhos\n",
        "    all_headers = []\n",
        "\n",
        "    # Itera sobre todos os arquivos no diretório de datasets\n",
        "    for file_path in datasets_dir.glob(\"*.csv\"):\n",
        "        try:\n",
        "            # Tentando múltiplas codificações\n",
        "            encodings = [\"utf-8\", \"ISO-8859-1\", \"latin1\"]\n",
        "            for encoding in encodings:\n",
        "                try:\n",
        "                    # Lê apenas o cabeçalho para evitar carregar grandes arquivos\n",
        "                    df = pd.read_csv(file_path, encoding=encoding, nrows=1)\n",
        "                    headers = df.columns.tolist()\n",
        "\n",
        "                    # Se for um arquivo vazio, ignora\n",
        "                    if not headers:\n",
        "                        print(f\"Aviso: {file_path.name} está vazio. Ignorando.\")\n",
        "                        break\n",
        "\n",
        "                    # Adiciona ao JSON final\n",
        "                    all_headers.append({file_path.name: headers})\n",
        "                    print(f\"✔ {file_path.name} carregado com sucesso.\")\n",
        "                    break\n",
        "                except UnicodeDecodeError:\n",
        "                    continue  # Tenta a próxima codificação\n",
        "            else:\n",
        "                print(f\"Erro: Não foi possível ler {file_path.name} com as codificações testadas.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao processar {file_path.name}: {e}\")\n",
        "\n",
        "    # Define o caminho de saída do JSON\n",
        "    output_path = datasets_dir.parent / \"headers.json\"\n",
        "\n",
        "    # Salva os cabeçalhos em um arquivo JSON\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as json_file:\n",
        "        json.dump(all_headers, json_file, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\n✅ Cabeçalhos salvos em {output_path}\")\n",
        "    print(\"Processo concluído.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}